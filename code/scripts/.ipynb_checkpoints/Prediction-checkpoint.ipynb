{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHgUAp48qwoG",
    "outputId": "411d4df6-d91a-42d4-819e-9cf641c12248"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['/home/benksy/Projects/LOPR_CoRL/scripts/.ipynb_checkpoints', '', '/home/benksy/catkin_ws/devel/lib/python3/dist-packages', '/opt/ros/noetic/lib/python3/dist-packages', '/home/benksy/miniconda3/envs/ldm/lib/python38.zip', '/home/benksy/miniconda3/envs/ldm/lib/python3.8', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/lib-dynload', '/home/benksy/.local/lib/python3.8/site-packages', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages', '/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers', '/home/benksy/Projects/OpenSTL', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/xarray-0.19.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/timm-0.9.2-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/scikit_learn-1.2.2-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/netCDF4-1.6.3-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/nni-3.0rc1-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/hickle-5.0.2-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/fvcore-0.1.5.post20221221-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/dask-2023.5.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/safetensors-0.3.1-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/huggingface_hub-0.14.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/threadpoolctl-3.1.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/cftime-1.6.2-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/websockets-11.0.3-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/typeguard-4.0.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/schema-0.7.5-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/responses-0.23.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/PythonWebHDFS-0.2.3-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/prettytable-3.7.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/nvidia_ml_py-11.525.112-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/json_tricks-3.17.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/colorama-0.4.6-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/cloudpickle-2.2.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/astor-0.8.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/h5py-3.8.0-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/iopath-0.1.10-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/tabulate-0.9.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/yacs-0.1.8-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/partd-1.4.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/contextlib2-21.6.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/types_PyYAML-6.0.12.10-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/simplejson-3.19.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/portalocker-2.7.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/locket-1.0.0-py3.8.egg', '/home/benksy/Projects/LOPR_CoRL/']\n",
      "['/home/benksy/Projects/LOPR_CoRL/scripts/.ipynb_checkpoints', '', '/home/benksy/catkin_ws/devel/lib/python3/dist-packages', '/opt/ros/noetic/lib/python3/dist-packages', '/home/benksy/miniconda3/envs/ldm/lib/python38.zip', '/home/benksy/miniconda3/envs/ldm/lib/python3.8', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/lib-dynload', '/home/benksy/.local/lib/python3.8/site-packages', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages', '/home/benksy/Projects/OpenSTL', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/xarray-0.19.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/timm-0.9.2-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/scikit_learn-1.2.2-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/netCDF4-1.6.3-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/nni-3.0rc1-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/hickle-5.0.2-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/fvcore-0.1.5.post20221221-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/dask-2023.5.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/safetensors-0.3.1-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/huggingface_hub-0.14.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/threadpoolctl-3.1.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/cftime-1.6.2-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/websockets-11.0.3-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/typeguard-4.0.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/schema-0.7.5-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/responses-0.23.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/PythonWebHDFS-0.2.3-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/prettytable-3.7.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/nvidia_ml_py-11.525.112-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/json_tricks-3.17.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/colorama-0.4.6-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/cloudpickle-2.2.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/astor-0.8.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/h5py-3.8.0-py3.8-linux-x86_64.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/iopath-0.1.10-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/tabulate-0.9.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/yacs-0.1.8-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/partd-1.4.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/contextlib2-21.6.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/types_PyYAML-6.0.12.10-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/simplejson-3.19.1-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/portalocker-2.7.0-py3.8.egg', '/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/locket-1.0.0-py3.8.egg', '/home/benksy/Projects/LOPR_CoRL/']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('/home/benksy/Projects/LOPR_CoRL/')\n",
    "import matplotlib.pyplot as plt\n",
    "import os, sys\n",
    "# print(os.environ)\n",
    "print(sys.path)\n",
    "sys.path.remove('/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers')\n",
    "print(sys.path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = '/media/benksy/Storage_SSD_2TB/Results/LOPR_CORL/'\n",
    "MODELS_DIR = '/media/benksy/Storage_SSD_2TB/Results/LOPR_CORL/'\n",
    "models = os.listdir(models_dir)\n",
    "\n",
    "single_model = '2023-05-28T04-32-43_ogm-prediction-simvp-no-encdec'\n",
    "\n",
    "assert single_model in models,  f'{single_model} not in {models}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "#@title loading utils\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "from src.util import instantiate_from_config\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_dataset_from_config(config):\n",
    "    print(config.data)\n",
    "    data = instantiate_from_config(config.data)    \n",
    "    data.prepare_data()\n",
    "    data.setup()\n",
    "    return data\n",
    "\n",
    "def get_model(single_model):\n",
    "    ckpt_path = os.listdir(os.path.join(MODELS_DIR, single_model, 'checkpoints'))\n",
    "    model_ckpts = []\n",
    "    # for ckpt in ckpt_path:\n",
    "    #     if ckpt == 'last.ckpt':\n",
    "    #         continue\n",
    "    #     print(ckpt)\n",
    "    #     model_ckpts.append(int(ckpt.split('=')[1].split('.')[0]))\n",
    "    best_ckpt_path = os.path.join(MODELS_DIR, single_model, 'checkpoints','last.ckpt') #f'epoch={max(model_ckpts):0{6}}.ckpt')\n",
    "    config_path = os.path.join(MODELS_DIR, single_model, 'configs', f'{single_model.split(\"_\")[0]}-project.yaml')\n",
    "    \n",
    "    config = OmegaConf.load(config_path)\n",
    "    model = load_model_from_config(config, best_ckpt_path)\n",
    "    data = load_dataset_from_config(config)\n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /media/benksy/Storage_SSD_2TB/Results/LOPR_CORL/2023-05-28T04-32-43_ogm-prediction-simvp-no-encdec/checkpoints/last.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading LatentPrediction from src.models.prediction.latent_prediction\n",
      "importlib:<module 'src.models.prediction.latent_prediction' from '/home/benksy/Projects/LOPR_CoRL/src/models/prediction/latent_prediction.py'>\n",
      "Loading SimVP_Model from src.modules.encoders.simvp_prediction\n",
      "importlib:<module 'src.modules.encoders.simvp_prediction' from '/home/benksy/Projects/LOPR_CoRL/src/modules/encoders/simvp_prediction.py'>\n",
      "######Warning: skip_encoder_decoder=True, hid_S=64.\n",
      "SimVP_Model has 6.33 M params.\n",
      "{'target': 'main.DataModuleFromConfig', 'params': {'batch_size': 64, 'train_batch_size': 64, 'val_batch_size': 16, 'num_workers': 23, 'wrap': False, 'train': {'target': 'src.data.nuscenes_ogm.NuscenesSeq', 'params': {'path': '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_PNG_NEW/train', 'path_latents': '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_Latents/train', 'past_len': 5, 'future_len': 15, 'step': 2, 'size': 128}}, 'validation': {'target': 'src.data.nuscenes_ogm.NuscenesSeq', 'params': {'path': '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_PNG_NEW/val', 'path_latents': '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_Latents/val', 'past_len': 5, 'future_len': 15, 'step': 2, 'size': 128}}}}\n",
      "Loading DataModuleFromConfig from main\n",
      "importlib:<module 'main' from '/home/benksy/Projects/LOPR_CoRL/main.py'>\n",
      "Loading NuscenesSeq from src.data.nuscenes_ogm\n",
      "importlib:<module 'src.data.nuscenes_ogm' from '/home/benksy/Projects/LOPR_CoRL/src/data/nuscenes_ogm.py'>\n",
      "Length of the dataset: 122519\n",
      "Loading NuscenesSeq from src.data.nuscenes_ogm\n",
      "importlib:<module 'src.data.nuscenes_ogm' from '/home/benksy/Projects/LOPR_CoRL/src/data/nuscenes_ogm.py'>\n",
      "Length of the dataset: 26216\n",
      "Loading NuscenesSeq from src.data.nuscenes_ogm\n",
      "importlib:<module 'src.data.nuscenes_ogm' from '/home/benksy/Projects/LOPR_CoRL/src/data/nuscenes_ogm.py'>\n",
      "Length of the dataset: 122519\n",
      "Loading NuscenesSeq from src.data.nuscenes_ogm\n",
      "importlib:<module 'src.data.nuscenes_ogm' from '/home/benksy/Projects/LOPR_CoRL/src/data/nuscenes_ogm.py'>\n",
      "Length of the dataset: 26216\n"
     ]
    }
   ],
   "source": [
    "model, data = get_model(single_model)\n",
    "val_dataloader = data._val_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfor\u001b[39;00m sample \u001b[39min\u001b[39;00m val_dataloader:\n\u001b[1;32m      2\u001b[0m     \u001b[39mpass\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/utils/data/dataloader.py:647\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    643\u001b[0m         warn_msg \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39m\"\u001b[39m\u001b[39mFor multiprocessing data-loading, this could be caused by not properly configuring the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    644\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mIterableDataset replica at each worker. Please see \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    645\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mhttps://pytorch.org/docs/stable/data.html#torch.utils.data.IterableDataset for examples.\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    646\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(warn_msg)\n\u001b[0;32m--> 647\u001b[0m \u001b[39mreturn\u001b[39;00m data\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for sample in val_dataloader:\n",
    "    pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_z, future_z = model.get_input(sample, model.encoder_stage_key, model.prediction_stage_key, model.latent_stage_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 5, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(obs_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 15, 64, 4, 4])\n"
     ]
    }
   ],
   "source": [
    "print(future_z.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([ 1.4266, -0.5501,  1.4424,  4.5062], device='cuda:0')\n",
      "tensor([ 1.5128, -0.9629,  1.6145,  4.1225], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print(obs_z[0,-1,0,0])\n",
    "print(future_z[0,0,0,0])\n",
    "\n",
    "pred_z = model.predict(obs_z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.3194, device='cuda:0', grad_fn=<MeanBackward0>)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_z - future_z)**2).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0723, 0.0993, 0.1311, 0.1599, 0.1940, 0.2437, 0.2870, 0.3320, 0.3663,\n",
       "        0.3969, 0.4446, 0.4799, 0.5111, 0.5320, 0.5415], device='cuda:0',\n",
       "       grad_fn=<MeanBackward1>)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "((pred_z - future_z)**2).mean((0,2,3,4))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "iIEAhY8AhUrh"
   },
   "source": [
    "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(input, n_samples=4):\n",
    "    # for i in range(1):\n",
    "    sample_1, all_output, all_obs, preds, all_future = sample_from_diffusion_model(input)\n",
    "    all_samples = [sample_1]\n",
    "    all_preds = [preds]\n",
    "    for _ in range(n_samples-1):\n",
    "        sample_, _, _, preds, _ = sample_from_diffusion_model(input)\n",
    "        all_samples.append(sample_)\n",
    "        all_preds.append(preds)\n",
    "\n",
    "            \n",
    "    output_samples = []\n",
    "    \n",
    "    for sample in all_samples:\n",
    "    \n",
    "        # Samples\n",
    "        grid = torch.stack(sample, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=2)\n",
    "        samples_grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        output_samples.append(samples_grid)\n",
    "    \n",
    "    # Ground truth\n",
    "    grid = torch.stack(all_output, 0)\n",
    "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "    grid = make_grid(grid, nrow=2)\n",
    "    gt_grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "    \n",
    "    # Obs\n",
    "    obs_grid = torch.stack(all_obs, 0).cpu().numpy()\n",
    "    obs_grid = 255* np.transpose(obs_grid, (0,1,2,4,5,3))[0,0]\n",
    "\n",
    "    # Future\n",
    "    future_grid = torch.stack(all_future, 0).cpu().numpy()\n",
    "    future_grid = 255* np.transpose(future_grid, (0,1,2,4,5,3))[0,0]\n",
    "\n",
    "    # Preds\n",
    "    pred_grids = []\n",
    "    for pred in all_preds:\n",
    "        pred_grid = torch.stack(pred, 0).cpu().numpy()\n",
    "        pred_grid = 255 * np.transpose(pred_grid, (0,1,2,4,5,3))[0,0]\n",
    "        pred_grids.append(pred_grid)\n",
    "\n",
    "    return output_samples, gt_grid, obs_grid, future_grid, pred_grids\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:135: UserWarning: TypedStorage is deprecated. It will be removed in the future and UntypedStorage will be the only storage class. This should only matter to you if you are using storages directly.  To access UntypedStorage directly, use tensor.untyped_storage() instead of tensor.storage()\n",
      "  storage = elem.storage()._new_shared(numel)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [491520], which does not match the required output shape [2, 5, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [1474560], which does not match the required output shape [2, 15, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n",
      "/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/taming/data/utils.py:137: UserWarning: An output with one or more elements was resized since it had shape [98304], which does not match the required output shape [2, 128, 128, 3]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at ../aten/src/ATen/native/Resize.cpp:26.)\n",
      "  return torch.stack(batch, 0, out=out)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "dict_keys(['past', 'future', 'image'])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1939136/1727624525.py:19: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xc = torch.tensor(input[1])[0][None]\n",
      "/tmp/ipykernel_1939136/1727624525.py:20: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  xf = torch.tensor(input[7])[0][None]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape for DDIM sampling is (1, 64, 4, 4), eta 1.0\n",
      "Running DDIM Sampling with 500 timesteps\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "DDIM Sampler:  51%|█████████████▋             | 254/500 [00:05<00:04, 50.36it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(sample\u001b[38;5;241m.\u001b[39mkeys())\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_input(sample, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mimage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpast\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfuture\u001b[39m\u001b[38;5;124m'\u001b[39m, return_future_inputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_first_stage_outputs\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m---> 12\u001b[0m output_samples, gt_grid, obs_grid, future_grid, pred_grids \u001b[38;5;241m=\u001b[39m \u001b[43mgenerate_img\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     14\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(num\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, nrows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mlen\u001b[39m(pred_grids), ncols\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m16\u001b[39m,\u001b[38;5;241m4\u001b[39m))\n\u001b[1;32m     16\u001b[0m axes[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(gt_grid\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39muint8))\n",
      "Cell \u001b[0;32mIn[7], line 3\u001b[0m, in \u001b[0;36mgenerate_img\u001b[0;34m(input, n_samples)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgenerate_img\u001b[39m(\u001b[38;5;28minput\u001b[39m, n_samples\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m):\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# for i in range(1):\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     sample_1, all_output, all_obs, preds, all_future \u001b[38;5;241m=\u001b[39m \u001b[43msample_from_diffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m     all_samples \u001b[38;5;241m=\u001b[39m [sample_1]\n\u001b[1;32m      5\u001b[0m     all_preds \u001b[38;5;241m=\u001b[39m [preds]\n",
      "Cell \u001b[0;32mIn[6], line 26\u001b[0m, in \u001b[0;36msample_from_diffusion_model\u001b[0;34m(input)\u001b[0m\n\u001b[1;32m     22\u001b[0m c \u001b[38;5;241m=\u001b[39m c[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m     24\u001b[0m all_future\u001b[38;5;241m.\u001b[39mappend(xf)\n\u001b[0;32m---> 26\u001b[0m samples_ddim, _ \u001b[38;5;241m=\u001b[39m \u001b[43msampler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mS\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mconditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     29\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mshape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     30\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m     31\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     32\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     33\u001b[0m \u001b[43m                                 \u001b[49m\u001b[43meta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_eta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# print(samples_ddim.shape, c.shape, xf.shape) #torch.Size([1, 64, 4, 4]) torch.Size([1, 5, 1024]) torch.Size([1, 15, 1024])\u001b[39;00m\n\u001b[1;32m     37\u001b[0m decoded_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict_and_decode(samples_ddim, c)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/models/diffusion/ddim.py:95\u001b[0m, in \u001b[0;36mDDIMSampler.sample\u001b[0;34m(self, S, batch_size, shape, conditioning, callback, normals_sequence, img_callback, quantize_x0, eta, mask, x0, temperature, noise_dropout, score_corrector, corrector_kwargs, verbose, x_T, log_every_t, unconditional_guidance_scale, unconditional_conditioning, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m size \u001b[38;5;241m=\u001b[39m (batch_size, C, H, W)\n\u001b[1;32m     93\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mData shape for DDIM sampling is \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, eta \u001b[39m\u001b[38;5;132;01m{\u001b[39;00meta\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 95\u001b[0m samples, intermediates \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mddim_sampling\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconditioning\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     96\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mimg_callback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mimg_callback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     98\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_x0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     99\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mmask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx0\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx0\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    100\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    101\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    102\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    103\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    104\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    105\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mx_T\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mx_T\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    106\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43mlog_every_t\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_every_t\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    107\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m                                            \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m samples, intermediates\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/models/diffusion/ddim.py:148\u001b[0m, in \u001b[0;36mDDIMSampler.ddim_sampling\u001b[0;34m(self, cond, shape, x_T, ddim_use_original_steps, callback, timesteps, quantize_denoised, mask, x0, img_callback, log_every_t, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[1;32m    145\u001b[0m     img_orig \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mq_sample(x0, ts)  \u001b[38;5;66;03m# TODO: deterministic forward pass?\u001b[39;00m\n\u001b[1;32m    146\u001b[0m     img \u001b[38;5;241m=\u001b[39m img_orig \u001b[38;5;241m*\u001b[39m mask \u001b[38;5;241m+\u001b[39m (\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m-\u001b[39m mask) \u001b[38;5;241m*\u001b[39m img\n\u001b[0;32m--> 148\u001b[0m outs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mp_sample_ddim\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcond\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muse_original_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mddim_use_original_steps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mquantize_denoised\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mquantize_denoised\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtemperature\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mnoise_dropout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnoise_dropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscore_corrector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mscore_corrector\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                          \u001b[49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcorrector_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_guidance_scale\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m                          \u001b[49m\u001b[43munconditional_conditioning\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43munconditional_conditioning\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    154\u001b[0m img, pred_x0 \u001b[38;5;241m=\u001b[39m outs\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m callback: callback(i)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/models/diffusion/ddim.py:176\u001b[0m, in \u001b[0;36mDDIMSampler.p_sample_ddim\u001b[0;34m(self, x, c, t, index, repeat_noise, use_original_steps, quantize_denoised, temperature, noise_dropout, score_corrector, corrector_kwargs, unconditional_guidance_scale, unconditional_conditioning)\u001b[0m\n\u001b[1;32m    174\u001b[0m     t_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([t] \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    175\u001b[0m     c_in \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([unconditional_conditioning, c])\n\u001b[0;32m--> 176\u001b[0m     e_t_uncond, e_t \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc_in\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mchunk(\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m    177\u001b[0m     e_t \u001b[38;5;241m=\u001b[39m e_t_uncond \u001b[38;5;241m+\u001b[39m unconditional_guidance_scale \u001b[38;5;241m*\u001b[39m (e_t \u001b[38;5;241m-\u001b[39m e_t_uncond)\n\u001b[1;32m    179\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m score_corrector \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/models/diffusion/ddpm.py:1174\u001b[0m, in \u001b[0;36mLatentDiffusion.apply_model\u001b[0;34m(self, x_noisy, t, cond, return_ids)\u001b[0m\n\u001b[1;32m   1171\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m fold(o) \u001b[38;5;241m/\u001b[39m normalization\n\u001b[1;32m   1173\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1174\u001b[0m     x_recon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_noisy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mcond\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1176\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x_recon, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m return_ids:\n\u001b[1;32m   1177\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x_recon[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/models/diffusion/ddpm.py:1628\u001b[0m, in \u001b[0;36mDiffusionWrapper.forward\u001b[0;34m(self, x, t, c_concat, c_crossattn)\u001b[0m\n\u001b[1;32m   1626\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcrossattn\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1627\u001b[0m     cc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat(c_crossattn, \u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m-> 1628\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiffusion_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcc\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1629\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconditioning_key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhybrid\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m   1630\u001b[0m     xc \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat([x] \u001b[38;5;241m+\u001b[39m c_concat, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/diffusionmodules/openaimodel.py:732\u001b[0m, in \u001b[0;36mUNetModel.forward\u001b[0;34m(self, x, timesteps, context, y, **kwargs)\u001b[0m\n\u001b[1;32m    730\u001b[0m h \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mtype(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[1;32m    731\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_blocks):\n\u001b[0;32m--> 732\u001b[0m     h \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43memb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    733\u001b[0m     hs\u001b[38;5;241m.\u001b[39mappend(h)\n\u001b[1;32m    734\u001b[0m h \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmiddle_block(h, emb, context)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/diffusionmodules/openaimodel.py:85\u001b[0m, in \u001b[0;36mTimestepEmbedSequential.forward\u001b[0;34m(self, x, emb, context)\u001b[0m\n\u001b[1;32m     83\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x, emb)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(layer, SpatialTransformer):\n\u001b[0;32m---> 85\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     87\u001b[0m     x \u001b[38;5;241m=\u001b[39m layer(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/attention.py:258\u001b[0m, in \u001b[0;36mSpatialTransformer.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    256\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb c h w -> b (h w) c\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    257\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m block \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_blocks:\n\u001b[0;32m--> 258\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[43mblock\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m x \u001b[38;5;241m=\u001b[39m rearrange(x, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb (h w) c -> b c h w\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh, w\u001b[38;5;241m=\u001b[39mw)\n\u001b[1;32m    260\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mproj_out(x)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/attention.py:209\u001b[0m, in \u001b[0;36mBasicTransformerBlock.forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 209\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcheckpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheckpoint\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/diffusionmodules/util.py:114\u001b[0m, in \u001b[0;36mcheckpoint\u001b[0;34m(func, inputs, params, flag)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m flag:\n\u001b[1;32m    113\u001b[0m     args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(inputs) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mtuple\u001b[39m(params)\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mCheckpointFunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39minputs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/autograd/function.py:506\u001b[0m, in \u001b[0;36mFunction.apply\u001b[0;34m(cls, *args, **kwargs)\u001b[0m\n\u001b[1;32m    503\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_C\u001b[38;5;241m.\u001b[39m_are_functorch_transforms_active():\n\u001b[1;32m    504\u001b[0m     \u001b[38;5;66;03m# See NOTE: [functorch vjp and autograd interaction]\u001b[39;00m\n\u001b[1;32m    505\u001b[0m     args \u001b[38;5;241m=\u001b[39m _functorch\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39munwrap_dead_wrappers(args)\n\u001b[0;32m--> 506\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m    508\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39msetup_context \u001b[38;5;241m==\u001b[39m _SingleLevelFunction\u001b[38;5;241m.\u001b[39msetup_context:\n\u001b[1;32m    509\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m    510\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIn order to use an autograd.Function with functorch transforms \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    511\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(vmap, grad, jvp, jacrev, ...), it must override the setup_context \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    512\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mstaticmethod. For more details, please see \u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    513\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://pytorch.org/docs/master/notes/extending.func.html\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/diffusionmodules/util.py:127\u001b[0m, in \u001b[0;36mCheckpointFunction.forward\u001b[0;34m(ctx, run_function, length, *args)\u001b[0m\n\u001b[1;32m    124\u001b[0m ctx\u001b[38;5;241m.\u001b[39minput_params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(args[length:])\n\u001b[1;32m    126\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 127\u001b[0m     output_tensors \u001b[38;5;241m=\u001b[39m \u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_function\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m output_tensors\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/attention.py:212\u001b[0m, in \u001b[0;36mBasicTransformerBlock._forward\u001b[0;34m(self, x, context)\u001b[0m\n\u001b[1;32m    211\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m--> 212\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattn1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnorm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    213\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattn2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm2(x), context\u001b[38;5;241m=\u001b[39mcontext) \u001b[38;5;241m+\u001b[39m x\n\u001b[1;32m    214\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mff(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnorm3(x)) \u001b[38;5;241m+\u001b[39m x\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/Projects/MultiFutureReasoningLatentDiffusion/ldm/modules/attention.py:193\u001b[0m, in \u001b[0;36mCrossAttention.forward\u001b[0;34m(self, x, context, mask)\u001b[0m\n\u001b[1;32m    191\u001b[0m out \u001b[38;5;241m=\u001b[39m einsum(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mb i j, b j d -> b i d\u001b[39m\u001b[38;5;124m'\u001b[39m, attn, v)\n\u001b[1;32m    192\u001b[0m out \u001b[38;5;241m=\u001b[39m rearrange(out, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m(b h) n d -> b n (h d)\u001b[39m\u001b[38;5;124m'\u001b[39m, h\u001b[38;5;241m=\u001b[39mh)\n\u001b[0;32m--> 193\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_out\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/nn/modules/linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "out_dir = '/home/benksy/Projects/latent_diff_out_images'\n",
    "\n",
    "model, data = get_model(single_model)\n",
    "val_dataloader = data._val_dataloader()\n",
    "sampler = DDIMSampler(model)\n",
    "\n",
    "for _idx, sample in enumerate(val_dataloader):\n",
    "    print(_idx)\n",
    "    if _idx > 500:\n",
    "        break\n",
    "    \n",
    "    print(sample.keys())\n",
    "    input = model.get_input(sample, 'image', 'past', 'future', return_future_inputs=True, return_first_stage_outputs=True)\n",
    "\n",
    "\n",
    "    output_samples, gt_grid, obs_grid, future_grid, pred_grids = generate_img(input)\n",
    "\n",
    "    fig, axes = plt.subplots(num=1, nrows=2 + len(pred_grids), ncols=15, figsize=(16,4))\n",
    "\n",
    "    axes[0, 0].imshow(gt_grid.astype(np.uint8))\n",
    "    \n",
    "    for idx, grid in enumerate(output_samples):  \n",
    "        axes[0, idx + 5].imshow(grid.astype(np.uint8))\n",
    "        axes[0, idx + 5].set_title(f'Sample {1 + idx}')\n",
    "#     axes[0, 2].imshow(grid_2.astype(np.uint8))vvv\n",
    "#     axes[0, 3].imshow(grid_3.astype(np.uint8))\n",
    "#     axes[0, 4].imshow(grid_4.astype(np.uint8))\n",
    "\n",
    "    # axes[0, 0].set_title('Ground truth')\n",
    "    # axes[0, 5].set_title('Sample 1')\n",
    "    # axes[0, 6].set_title('Sample 2')\n",
    "    # axes[0, 7].set_title('Sample 3')\n",
    "    # axes[0, 8].set_title('Sample 4')\n",
    "\n",
    "    fig.suptitle('Last frame prediction (1.5 second into the future)', fontsize=16, y=1.0)\n",
    "\n",
    "    axes[0, 0].imshow(obs_grid[0].astype(np.uint8))\n",
    "    axes[0, 1].imshow(obs_grid[1].astype(np.uint8))\n",
    "    axes[0, 2].imshow(obs_grid[2].astype(np.uint8))\n",
    "    axes[0, 3].imshow(obs_grid[3].astype(np.uint8))\n",
    "    axes[0, 4].imshow(obs_grid[4].astype(np.uint8))\n",
    "\n",
    "    axes[0, 0].set_title('Obs 1')\n",
    "    axes[0, 1].set_title('Obs 2')\n",
    "    axes[0, 2].set_title('Obs 3')\n",
    "    axes[0, 3].set_title('Obs 4')\n",
    "    axes[0, 4].set_title('Obs 5')\n",
    "\n",
    "    for t in range(15):\n",
    "        axes[1, t].imshow(future_grid[t].astype(np.uint8))\n",
    "\n",
    "    for idx, pred_grid in enumerate(pred_grids):\n",
    "        for t in range(15):\n",
    "            axes[2 + idx, t].imshow(pred_grid[t].astype(np.uint8))\n",
    "            axes[2 + idx, t].set_title(f'Pred {idx+1}')\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "    \n",
    "    plt.show()\n",
    "    # plt.savefig(f'{out_dir}/Sample_{_idx}.png')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
