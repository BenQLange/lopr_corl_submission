{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NUmmV5ZvrPbP"
   },
   "source": [
    "# Class-Conditional Synthesis with Latent Diffusion Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zh7u8gOx0ivw"
   },
   "source": [
    "Install all the requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tWAqdwk0Nrn"
   },
   "source": [
    "Load it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "cellView": "form",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NHgUAp48qwoG",
    "outputId": "411d4df6-d91a-42d4-819e-9cf641c12248"
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\".\")\n",
    "sys.path.append('../')\n",
    "sys.path.append('/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/')\n",
    "sys.path.append('/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/')\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'DataModuleFromConfig' from 'main' (/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/main.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [2], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mmain\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m DataModuleFromConfig\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'DataModuleFromConfig' from 'main' (/home/benksy/Projects/MultiFutureReasoningLatentDiffusion/src/taming-transformers/main.py)"
     ]
    }
   ],
   "source": [
    "from main import DataModuleFromConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNqCqQDoyZmq"
   },
   "source": [
    "Now, download the checkpoint (~1.7 GB). This will usually take 1-2 minutes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThxmCePqt1mt"
   },
   "source": [
    "Let's also check what type of GPU we've got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "form",
    "id": "fnGwQRhtyBhb"
   },
   "outputs": [],
   "source": [
    "#@title loading utils\n",
    "import torch\n",
    "from omegaconf import OmegaConf\n",
    "\n",
    "from ldm.util import instantiate_from_config\n",
    "\n",
    "\n",
    "def load_model_from_config(config, ckpt):\n",
    "    print(f\"Loading model from {ckpt}\")\n",
    "    pl_sd = torch.load(ckpt)#, map_location=\"cpu\")\n",
    "    sd = pl_sd[\"state_dict\"]\n",
    "    model = instantiate_from_config(config.model)\n",
    "    m, u = model.load_state_dict(sd, strict=False)\n",
    "    model.cuda()\n",
    "    model.eval()\n",
    "    return model\n",
    "\n",
    "def load_dataset_from_config(config):\n",
    "    data = instantiate_from_config(config.data)    \n",
    "    data.prepare_data()\n",
    "    data.setup()\n",
    "    return data\n",
    "\n",
    "def get_model():\n",
    "    config = OmegaConf.load(\"/media/benksy/Storage_SSD_2TB/Results/latent_diffusion/2023-04-14T09-12-40_ogm-ldm/configs/2023-04-14T09-12-40-project.yaml\")  \n",
    "    #print(config['model'].keys())\n",
    "    #print(config['model']['params']['first_stage_config']['params']['ckpt_path'])\n",
    "    config['model']['params']['first_stage_config']['params']['ckpt_path']='/media/benksy/Storage_SSD_2TB_2/Results/LOPR-plusplus/nuscenes-ogm-drivegan-no-theme-gan-representation-learning-2023-03-16T21:31/230000.pt'\n",
    "    config['data']['params']['train']['params']['path'] = '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_PNG_NEW/train'\n",
    "    config['data']['params']['validation']['params']['path'] = '/media/benksy/Storage_SSD_2TB/Datasets/Nuscenes_OGM_PNG_NEW/val'\n",
    "    model = load_model_from_config(config, \"/media/benksy/Storage_SSD_2TB/Results/latent_diffusion/2023-04-14T09-12-40_ogm-ldm/checkpoints/last.ckpt\")\n",
    "    data = load_dataset_from_config(config)\n",
    "    return model, data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BPnyd-XUKbfE",
    "outputId": "0fcd10e4-0df2-4ab9-cbf5-f08f4902c954"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from /media/benksy/Storage_SSD_2TB/Results/latent_diffusion/2023-04-14T09-12-40_ogm-ldm/checkpoints/last.ckpt\n",
      "ldm.models.diffusion.ddpm.LatentDiffusion\n",
      "LatentDiffusion: Running in eps-prediction mode\n",
      "ldm.modules.diffusionmodules.openaimodel.UNetModel\n",
      "DiffusionWrapper has 106.63 M params.\n",
      "Keeping EMAs of 628.\n",
      "ldm.models.autoencoder_ogm.VAEGANModelInterface\n",
      "4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using /home/benksy/.cache/torch_extensions/py38_cu117 as PyTorch extensions root...\n",
      "Creating extension directory /home/benksy/.cache/torch_extensions/py38_cu117/fused...\n",
      "Detected CUDA files, patching ldflags\n",
      "Emitting ninja build file /home/benksy/.cache/torch_extensions/py38_cu117/fused/build.ninja...\n",
      "Building extension module fused...\n",
      "Allowing ninja to set a default number of workers... (overridable by setting the environment variable MAX_JOBS=N)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/3] c++ -MMD -MF fused_bias_act.o.d -DTORCH_EXTENSION_NAME=fused -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/TH -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.1/include -isystem /home/benksy/miniconda3/envs/ldm/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -fPIC -std=c++17 -c /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp -o fused_bias_act.o \n",
      "In file included from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:11,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                 from /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:1:\n",
      "/home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp: In function ‘at::Tensor fused_bias_act(const at::Tensor&, const at::Tensor&, const at::Tensor&, int, int, float, float)’:\n",
      "/home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:7:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    7 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "      |                                          ^\n",
      "/home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:13:5: note: in expansion of macro ‘CHECK_CUDA’\n",
      "   13 |     CHECK_CUDA(input);\n",
      "      |     ^~~~~~~~~~\n",
      "In file included from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                 from /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:1:\n",
      "/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
      "  222 |   DeprecatedTypeProperties & type() const {\n",
      "      |                              ^~~~\n",
      "In file included from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/c10/core/DeviceType.h:8,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/c10/core/Device.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:11,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                 from /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:1:\n",
      "/home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:7:42: warning: ‘at::DeprecatedTypeProperties& at::Tensor::type() const’ is deprecated: Tensor.type() is deprecated. Instead use Tensor.options(), which in many cases (e.g. in a constructor) is a drop-in replacement. If you were using data from type(), that is now available from Tensor itself, so instead of tensor.type().scalar_type(), use tensor.scalar_type() instead and instead of tensor.type().backend() use tensor.device(). [-Wdeprecated-declarations]\n",
      "    7 | #define CHECK_CUDA(x) TORCH_CHECK(x.type().is_cuda(), #x \" must be a CUDA tensor\")\n",
      "      |                                          ^\n",
      "/home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:14:5: note: in expansion of macro ‘CHECK_CUDA’\n",
      "   14 |     CHECK_CUDA(bias);\n",
      "      |     ^~~~~~~~~~\n",
      "In file included from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/Tensor.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/function_hook.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/cpp_hook.h:2,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/variable.h:6,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/autograd/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/autograd.h:3,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include/torch/all.h:7,\n",
      "                 from /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/extension.h:4,\n",
      "                 from /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act.cpp:1:\n",
      "/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/ATen/core/TensorBody.h:222:30: note: declared here\n",
      "  222 |   DeprecatedTypeProperties & type() const {\n",
      "      |                              ^~~~\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/3] /usr/local/cuda-11.1/bin/nvcc  -DTORCH_EXTENSION_NAME=fused -DTORCH_API_INCLUDE_EXTENSION_H -DPYBIND11_COMPILER_TYPE=\\\"_gcc\\\" -DPYBIND11_STDLIB=\\\"_libstdcpp\\\" -DPYBIND11_BUILD_ABI=\\\"_cxxabi1011\\\" -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/torch/csrc/api/include -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/TH -isystem /home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/include/THC -isystem /usr/local/cuda-11.1/include -isystem /home/benksy/miniconda3/envs/ldm/include/python3.8 -D_GLIBCXX_USE_CXX11_ABI=0 -D__CUDA_NO_HALF_OPERATORS__ -D__CUDA_NO_HALF_CONVERSIONS__ -D__CUDA_NO_BFLOAT16_CONVERSIONS__ -D__CUDA_NO_HALF2_OPERATORS__ --expt-relaxed-constexpr -gencode=arch=compute_75,code=compute_75 -gencode=arch=compute_75,code=sm_75 --compiler-options '-fPIC' -std=c++17 -c /home/benksy/Projects/LOPR++/src/representation_learning/op/fused_bias_act_kernel.cu -o fused_bias_act_kernel.cuda.o \n",
      "[3/3] c++ fused_bias_act.o fused_bias_act_kernel.cuda.o -shared -L/home/benksy/miniconda3/envs/ldm/lib/python3.8/site-packages/torch/lib -lc10 -lc10_cuda -ltorch_cpu -ltorch_cuda -ltorch -ltorch_python -L/usr/local/cuda-11.1/lib64 -lcudart -o fused.so\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading extension module fused...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ldm.modules.encoders.modules.TransformerEmbedder\n",
      "main.DataModuleFromConfig\n",
      "Length of the dataset: 122519\n",
      "Length of the dataset: 26216\n",
      "Length of the dataset: 122519\n",
      "Length of the dataset: 26216\n"
     ]
    }
   ],
   "source": [
    "from ldm.models.diffusion.ddim import DDIMSampler\n",
    "\n",
    "model, data = get_model()\n",
    "sampler = DDIMSampler(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = data._val_dataloader()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iIEAhY8AhUrh"
   },
   "source": [
    "And go. Quality, sampling speed and diversity are best controlled via the `scale`, `ddim_steps` and `ddim_eta` variables. As a rule of thumb, higher values of `scale` produce better samples at the cost of a reduced output diversity. Furthermore, increasing `ddim_steps` generally also gives higher quality samples, but returns are diminishing for values > 250. Fast sampling (i e. low values of `ddim_steps`) while retaining good quality can be achieved by using `ddim_eta = 0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "jcbqWX2Ytu9t",
    "outputId": "3b7adde0-d80e-4c01-82d2-bf988aee7455"
   },
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "from PIL import Image\n",
    "from einops import rearrange\n",
    "from torchvision.utils import make_grid\n",
    "\n",
    "def sample_from_diffusion_model(input):\n",
    "    ddim_steps = 500\n",
    "    ddim_eta = 1.0\n",
    "    scale = 10.0  \n",
    "\n",
    "    shape = (model.channels, model.image_size, model.image_size)\n",
    "    all_samples = list()\n",
    "    all_output = list()\n",
    "    all_obs = list()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        with model.ema_scope():\n",
    "            xc = torch.tensor(input[1])[0][None]\n",
    "#             print(xc.shape)\n",
    "            c = model.get_learned_conditioning(xc.to(model.device))\n",
    "\n",
    "#             print(c.shape)\n",
    "\n",
    "            c = c[0][None]\n",
    "\n",
    "#             print(f'C:{c.shape}')\n",
    "\n",
    "            samples_ddim, _ = sampler.sample(S=ddim_steps,\n",
    "                                             conditioning=c,\n",
    "                                             batch_size=1,\n",
    "                                             shape=shape,\n",
    "                                             verbose=False,\n",
    "                                             unconditional_conditioning=c,\n",
    "                                             unconditional_guidance_scale=scale,\n",
    "                                             eta=ddim_eta)\n",
    "\n",
    "            x_samples_ddim = model.decode_first_stage(samples_ddim)\n",
    "            target_output = model.decode_first_stage(input[0][0][None])\n",
    "            xc = xc.reshape(5, 64, 4, 4)\n",
    "            obs = model.decode_first_stage(xc)\n",
    "            x_samples_ddim = torch.clamp((x_samples_ddim+1.0)/2.0, \n",
    "                                         min=0.0, max=1.0)\n",
    "            target_output = torch.clamp((target_output+1.0)/2.0, \n",
    "                                         min=0.0, max=1.0)\n",
    "\n",
    "            obs = torch.clamp((obs+1.0)/2.0, \n",
    "                                         min=0.0, max=1.0)\n",
    "            obs = obs.reshape(1, 5, 3, 128, 128)\n",
    "            all_obs.append(obs)\n",
    "            all_samples.append(x_samples_ddim)\n",
    "            all_output.append(target_output)\n",
    "            \n",
    "        return all_samples, all_output, all_obs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_img(input):\n",
    "    # for i in range(1):\n",
    "    sample_1, all_output, all_obs = sample_from_diffusion_model(input)\n",
    "    sample_2 = sample_from_diffusion_model(input)[0]\n",
    "    sample_3 = sample_from_diffusion_model(input)[0]\n",
    "    sample_4 = sample_from_diffusion_model(input)[0]\n",
    "    \n",
    "    all_samples = [sample_1, sample_2, sample_3, sample_4]\n",
    "    output_samples = []\n",
    "    \n",
    "    for sample in all_samples:\n",
    "    \n",
    "        # Samples\n",
    "        grid = torch.stack(sample, 0)\n",
    "        grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "        grid = make_grid(grid, nrow=2)\n",
    "        samples_grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "        output_samples.append(samples_grid)\n",
    "    \n",
    "    # Ground truth\n",
    "    grid = torch.stack(all_output, 0)\n",
    "    grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "    grid = make_grid(grid, nrow=2)\n",
    "    gt_grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "    \n",
    "    # Obs\n",
    "    obs_grid = torch.stack(all_obs, 0).cpu().numpy()\n",
    "    obs_grid = 255* np.transpose(obs_grid, (0,1,2,4,5,3))[0,0]\n",
    "\n",
    "    return output_samples, gt_grid, obs_grid\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# for i in range(10000):\n",
    "for idx, sample in enumerate(val_dataloader):\n",
    "    if idx > 500:\n",
    "        break\n",
    "    \n",
    "#     sample = next(iter(val_dataloader))\n",
    "#     if i%500!=0:\n",
    "#         continue\n",
    "    # for key in sample.keys():\n",
    "    #     print(f'{key}:{sample[key].shape}')\n",
    "    input = model.get_input(sample, 'image')\n",
    "    # print(input[0].shape, input[1].shape)\n",
    "\n",
    "    output_samples, gt_grid, obs_grid = generate_img(input)\n",
    "\n",
    "    fig, axes = plt.subplots(num=1, nrows=2, ncols=5, figsize=(16,4))\n",
    "\n",
    "    axes[0, 0].imshow(gt_grid.astype(np.uint8))\n",
    "    \n",
    "    for idx, grid in enumerate(output_samples):  \n",
    "        axes[0, idx + 1].imshow(grid.astype(np.uint8))\n",
    "#     axes[0, 2].imshow(grid_2.astype(np.uint8))\n",
    "#     axes[0, 3].imshow(grid_3.astype(np.uint8))\n",
    "#     axes[0, 4].imshow(grid_4.astype(np.uint8))\n",
    "\n",
    "    axes[0, 0].set_title('Ground truth')\n",
    "    axes[0, 1].set_title('Sample 1')\n",
    "    axes[0, 2].set_title('Sample 2')\n",
    "    axes[0, 3].set_title('Sample 3')\n",
    "    axes[0, 4].set_title('Sample 4')\n",
    "\n",
    "    fig.suptitle('Last frame prediction (1.5 second into the future)', fontsize=16, y=1.0)\n",
    "\n",
    "    axes[1, 0].imshow(obs_grid[0].astype(np.uint8))\n",
    "    axes[1, 1].imshow(obs_grid[1].astype(np.uint8))\n",
    "    axes[1, 2].imshow(obs_grid[2].astype(np.uint8))\n",
    "    axes[1, 3].imshow(obs_grid[3].astype(np.uint8))\n",
    "    axes[1, 4].imshow(obs_grid[4].astype(np.uint8))\n",
    "\n",
    "    axes[1, 0].set_title('Obs 1')\n",
    "    axes[1, 1].set_title('Obs 2')\n",
    "    axes[1, 2].set_title('Obs 3')\n",
    "    axes[1, 3].set_title('Obs 4')\n",
    "    axes[1, 4].set_title('Obs 5')\n",
    "\n",
    "    for ax in axes.flatten():\n",
    "        ax.set_xticks([])\n",
    "        ax.set_yticks([])\n",
    "        ax.set_xlabel('')\n",
    "        ax.set_ylabel('')\n",
    "\n",
    "\n",
    "    # Adjust spacing between subplots\n",
    "    fig.subplots_adjust(hspace=0.3)\n",
    "\n",
    "#     plt.show()\n",
    "    plt.savefig(f'Sample_{idx}.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for i in range(1):\n",
    "all_samples, all_output, all_obs = sample_from_diffusion_model(input)\n",
    "\n",
    "# display as grid\n",
    "grid = torch.stack(all_samples, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=2)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples, all_output, all_obs = sample_from_diffusion_model(input)\n",
    "\n",
    "# display as grid\n",
    "grid = torch.stack(all_samples, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=2)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_samples, all_output, all_obs = sample_from_diffusion_model(input)\n",
    "\n",
    "# display as grid\n",
    "grid = torch.stack(all_samples, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=2)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display as grid\n",
    "grid = torch.stack(all_output, 0)\n",
    "grid = rearrange(grid, 'n b c h w -> (n b) c h w')\n",
    "grid = make_grid(grid, nrow=5)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid = torch.stack(all_obs, 0)\n",
    "grid = rearrange(grid, 'n b t c h w -> (n b t) c h w')\n",
    "grid = make_grid(grid, nrow=5)\n",
    "\n",
    "# to image\n",
    "grid = 255. * rearrange(grid, 'c h w -> h w c').cpu().numpy()\n",
    "Image.fromarray(grid.astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "latent-imagenet-diffusion.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
